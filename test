
case class urlData(LE_ID:String, PROFILE_NAME:String, SUB_SEGMENT_NAME:String, DF_INCORP_COUNTRY:String, RTT_CLEARED_DERIVATIVES:String, RTT_OTC_DERIVATIVES:String , RTT_REPO:String, RTT_FXSPOT:String, HAVE_ISDA_AGMT:String, HAVE_CAT2_DERV_LIMIT:String, HAVE_REPO_AGMT:String, HAVE_CAT2_REPO_LIMIT:String, HAVE_CAT3_LIMIT:String)
  
  def main(args: Array[String]){
    
    val conf = new SparkConf().setAppName("POC")
    val sc = new SparkContext(conf)
    val sqlContext = new org.apache.spark.sql.SQLContext(sc)
    val hc = new HiveContext(sc)
    
    val url = "url"
    
    // Fetching data from URL
    val content = scala.io.Source.fromURL(url).mkString
    
    val line = content.split("\n")
    val dataRdd = sc.parallelize(line)
    
    val header = dataRdd.first()
    val dataNoHeader = dataRdd.filter(x => (x != header))
    
    dataNoHeader.coalesce(1).saveAsTextFile("path")
    
    //making dataframe after reading data
    
    //val df = sqlContext.read.format("csv").option("header", "true").load(url)
    import sqlContext.implicits._
    
    val dataSplit = dataNoHeader.map {x =>
                                      val columns = x.split(",")
                                      urlData(columns(0), columns(1),	columns(2),	columns(3),	columns(4),	columns(5),	columns(6),	columns(7),	columns(8),	columns(9),	columns(10),	columns(11),	columns(12))}
    
    val urlDataDf = dataSplit.toDF()
    
    urlDataDf.limit(10).show()
    
